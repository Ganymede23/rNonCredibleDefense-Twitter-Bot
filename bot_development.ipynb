{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Authentication\n",
    "\n",
    "import tweepy\n",
    "from twitter_authentication import API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    "\n",
    "oauth1_user_handler = tweepy.OAuth1UserHandler(\n",
    "    API_KEY, API_SECRET,\n",
    "    callback=\"oob\"\n",
    ")\n",
    "\n",
    "print(oauth1_user_handler.get_authorization_url())\n",
    "\n",
    "verifier = input(\"Input PIN: \")\n",
    "access_token, access_token_secret = oauth1_user_handler.get_access_token(\n",
    "    verifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    "\n",
    "# API\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Client\n",
    "client = tweepy.Client(\n",
    "    consumer_key = API_KEY,\n",
    "    consumer_secret = API_SECRET,\n",
    "    access_token = ACCESS_TOKEN,\n",
    "    access_token_secret = ACCESS_TOKEN_SECRET,\n",
    ")\n",
    "\n",
    "# media = api.media_upload(\"Media/aeroGAVIN.jpg\")\n",
    "# print(media)\n",
    "\n",
    "# client.create_tweet(text='t4', media_ids=[media.media_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = api.media_upload(\"Media/aeroGAVIN.jpg\", possibly_sensitive=True)\n",
    "print(media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit authentication\n",
    "\n",
    "import praw\n",
    "from reddit_authentication import CLIENT_ID, SECRET_KEY, USER_AGENT\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = CLIENT_ID,\n",
    "    client_secret = SECRET_KEY,\n",
    "    user_agent = USER_AGENT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLITE\n",
    "\n",
    "import sqlite3\n",
    "con = sqlite3.connect(\"bot_db.db\")\n",
    "cur = con.cursor()\n",
    "cur.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS reddit_post (\n",
    "                    post_id     TEXT PRIMARY KEY,\n",
    "                    title       TEXT NOT NULL,\n",
    "                    flair       TEXT,\n",
    "                    post_url    TEXT NOT NULL,\n",
    "                    author      TEXT NOT NULL,\n",
    "                    post_date   TEXT NOT NULL,\n",
    "                    scrape_date TEXT NOT NULL,\n",
    "                    tweet_url   TEXT,\n",
    "                    tweet_date  TEXT\n",
    "            );''')\n",
    "cur.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS post_media (\n",
    "                    post_id     TEXT NOT NULL,\n",
    "                    url         TEXT NOT NULL,\n",
    "                    FOREIGN KEY (post_id) REFERENCES reddit_post (post_id)\n",
    "                        ON DELETE CASCADE\n",
    "            );''')\n",
    "\n",
    "\n",
    "def insert_into_table(db: str, table_name: str, data: list):\n",
    "    con = sqlite3.connect(db)\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    sql = f\"INSERT INTO {table_name} VALUES ({','.join(['?' for _ in data[0]])})\"\n",
    "    cur.executemany(sql, data)\n",
    "    \n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "def update_table(db: str, table_name: str, set_values_dict: dict, where_clause: str):\n",
    "    con = sqlite3.connect(db)\n",
    "    cur = con.cursor()\n",
    "    \n",
    "\n",
    "    set_clause = ', '.join([f\"{col} = ?\" for col in set_values_dict.keys()])\n",
    "    sql = f\"UPDATE {table_name} SET {set_clause} WHERE {where_clause}\"\n",
    "    cur.execute(sql, tuple(set_values_dict.values()))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded cell to test specific posts\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "post_data = []\n",
    "media_data = []\n",
    "\n",
    "submission = reddit.submission(url='https://www.reddit.com/r/dogelore/comments/jbu197/he_chose_poorly/')\n",
    "\n",
    "if not submission.stickied and not submission.is_self:\n",
    "    # print(f'Post: {submission.id, submission.name, submission.title, submission.selftext}')\n",
    "    # print(f'Flair: {submission.link_flair_text}')\n",
    "    # print(f'Status: {submission.over_18, submission.spoiler, submission.stickied, submission.created_utc}')\n",
    "    # print(f'Links: reddit.com{submission.permalink}, {submission.url}')\n",
    "    # print(f'Author: {submission.author.name}')\n",
    "    # print(submission.score)\n",
    "\n",
    "    post_date = datetime.fromtimestamp(submission.created_utc) + timedelta(hours = 3) # Adds 3 hours because local time is GMT-3. Review this.\n",
    "    datetime_now = datetime.now(timezone.utc)\n",
    "    scrape_date = datetime_now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    post_data_row = (\n",
    "            submission.id,                          # post_id     TEXT PRIMARY KEY,\n",
    "            submission.title,                       # title       TEXT NOT NULL,\n",
    "            submission.link_flair_text,             # flair       TEXT,\n",
    "            'reddit.com' + submission.permalink,    # post_url    TEXT NOT NULL,\n",
    "            submission.author.name,                 # author      TEXT NOT NULL,\n",
    "            submission.created_utc,                 # post_date   TEXT NOT NULL,\n",
    "            scrape_date,                            # scrape_date TEXT NOT NULL,\n",
    "            None,\n",
    "            None\n",
    "    )\n",
    "    post_data.append(post_data_row)\n",
    "\n",
    "    if hasattr(submission, 'gallery_data'):\n",
    "        for item in sorted(submission.gallery_data['items'], key=lambda x: x['id']):\n",
    "            media_id = item['media_id']\n",
    "            media_metadata = submission.media_metadata[media_id]\n",
    "            print(media_metadata['s']['u'])\n",
    "            media_data_row = (\n",
    "                    submission.id,                          # post_id     TEXT NOT NULL,\n",
    "                    media_metadata['s']['u'],               # url         TEXT NOT NULL,\n",
    "            )\n",
    "            # print(media_data_row)\n",
    "            media_data.append(media_data_row)\n",
    "    else:\n",
    "        media_data_row = (\n",
    "                submission.id,                          # post_id     TEXT NOT NULL,\n",
    "                submission.url,                         # url         TEXT NOT NULL,\n",
    "        )\n",
    "        media_data.append(media_data_row)\n",
    "\n",
    "insert_into_table('bot_db.db', 'reddit_post', post_data)\n",
    "insert_into_table('bot_db.db', 'post_media', media_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "post_data = []\n",
    "media_data = []\n",
    "\n",
    "for submission in reddit.subreddit(\"NonCredibleDefense\").top(limit = 5, time_filter = \"day\"):\n",
    "    if not submission.stickied and not submission.is_self:\n",
    "        # print(f'Post: {submission.id, submission.name, submission.title, submission.selftext}')\n",
    "        # print(f'Flair: {submission.link_flair_text}')\n",
    "        # print(f'Status: {submission.over_18, submission.spoiler, submission.stickied, submission.created_utc}')\n",
    "        # print(f'Links: reddit.com{submission.permalink}, {submission.url}')\n",
    "        # print(f'Author: {submission.author.name}')\n",
    "        # print(submission.score)\n",
    "\n",
    "        post_date = datetime.fromtimestamp(submission.created_utc) + timedelta(hours = 3) # Adds 3 hours because local time is GMT-3. Review this.\n",
    "        datetime_now = datetime.now(timezone.utc)\n",
    "        scrape_date = datetime_now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        post_data_row = (\n",
    "                submission.id,                          # post_id     TEXT PRIMARY KEY,\n",
    "                submission.title,                       # title       TEXT NOT NULL,\n",
    "                submission.link_flair_text,             # flair       TEXT,\n",
    "                'reddit.com' + submission.permalink,    # post_url    TEXT NOT NULL,\n",
    "                submission.author.name,                 # author      TEXT NOT NULL,\n",
    "                submission.created_utc,                 # post_date   TEXT NOT NULL,\n",
    "                scrape_date,                            # scrape_date TEXT NOT NULL,\n",
    "                None,\n",
    "                None\n",
    "        )\n",
    "        post_data.append(post_data_row)\n",
    "\n",
    "        if hasattr(submission, 'gallery_data'):\n",
    "            for item in sorted(submission.gallery_data['items'], key=lambda x: x['id']):\n",
    "                media_id = item['media_id']\n",
    "                media_metadata = submission.media_metadata[media_id]\n",
    "                # print(media_metadata['s']['u'])\n",
    "                media_data_row = (\n",
    "                        submission.id,                          # post_id     TEXT NOT NULL,\n",
    "                        media_metadata['s']['u'],               # url         TEXT NOT NULL,\n",
    "                )\n",
    "        else:\n",
    "            media_data_row = (\n",
    "                    submission.id,                          # post_id     TEXT NOT NULL,\n",
    "                    submission.url,                         # url         TEXT NOT NULL,\n",
    "            )\n",
    "        media_data.append(media_data_row)\n",
    "\n",
    "insert_into_table('bot_db.db', 'reddit_post', post_data)\n",
    "insert_into_table('bot_db.db', 'post_media', media_data)\n",
    "\n",
    "# tweet_url   TEXT,\n",
    "# tweet_date  TEXT\n",
    "\n",
    "        # title = submission.title\n",
    "        # flair = submission.link_flair_text\n",
    "        # if flair:\n",
    "        #     flair = flair.translate(str.maketrans('&', 'n', '!/ '))\n",
    "        # submission_url = 'reddit.com' + submission.permalink\n",
    "        # media_url = submission.url\n",
    "        # author = submission.author.name\n",
    "        # user_profile_url = 'reddit.com/u/' + user\n",
    "\n",
    "# https://praw.readthedocs.io/en/stable/getting_started/quick_start.html\n",
    "# https://github.com/reddit-archive/reddit/wiki/API#rules\n",
    "# https://docs.python.org/3/library/sqlite3.html\n",
    "\n",
    "# Posts to test:\n",
    "#   VIDEO: https://www.reddit.com/r/NonCredibleDefense/comments/11b2sxa/i_think_ukrainian_minister_of_defense_just/\n",
    "#   VIDEO: https://www.reddit.com/r/NonCredibleDefense/comments/t0o3lc/godspeed_ghost_of_kyiv_may_the_razgriz_be_with_you/\n",
    "#   GALLERY: https://www.reddit.com/r/NonCredibleDefense/comments/11bimqt/pov_its_20220225_second_day_kyiv_time_you_are/\n",
    "\n",
    "# To do: Prevent exceptions from happening when it attempts to write into the DB a post that is already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.reddit.com/r/dogelore/comments/jbu197/he_chose_poorly/'\n",
    "# submission = reddit.submission(url=url)\n",
    "\n",
    "url_list = [\n",
    "    'https://www.reddit.com/r/dogelore/comments/jbu197/he_chose_poorly/', \n",
    "    'https://www.reddit.com/r/NonCredibleDefense/comments/t0o3lc/godspeed_ghost_of_kyiv_may_the_razgriz_be_with_you/', \n",
    "    'https://www.reddit.com/r/NonCredibleDefense/comments/11bimqt/pov_its_20220225_second_day_kyiv_time_you_are/'\n",
    "]\n",
    "\n",
    "for item in url_list:\n",
    "    submission = reddit.submission(url=item)\n",
    "    print(hasattr(submission, 'gallery_data'))\n",
    "    # for item in sorted(submission.gallery_data['items'], key=lambda x: x['id']):\n",
    "    #     print(f'Post: {submission.id, submission.name, submission.title, submission.selftext}')\n",
    "    #     print(f'Flair: {submission.link_flair_text}')\n",
    "    #     print(f'Status: {submission.over_18, submission.spoiler, submission.stickied, submission.created_utc}')\n",
    "    #     print(f'Links: reddit.com{submission.permalink}, {submission.url}')\n",
    "    #     print(f'Author: {submission.author.name}')\n",
    "    #     print()\n",
    "\n",
    "# GALLERY CASE\n",
    "\n",
    "# url = 'https://www.reddit.com/r/NonCredibleDefense/comments/11bimqt/pov_its_20220225_second_day_kyiv_time_you_are/'\n",
    "url = 'https://www.reddit.com/r/dogelore/comments/jbu197/he_chose_poorly/'\n",
    "# url = 'https://www.reddit.com/r/NonCredibleDefense/comments/11pkssc/any_day_now/'\n",
    "submission = reddit.submission(url=url)\n",
    "# print(len(sorted(submission.gallery_data['items'], key=lambda x: x['id'])))\n",
    "for item in sorted(submission.gallery_data['items'], key=lambda x: x['id']):\n",
    "    media_id = item['media_id']\n",
    "    media_metadata = submission.media_metadata[media_id]\n",
    "    print(media_metadata['s']['u'])\n",
    "    print(type(media_metadata['s']['u']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('https://i.redd.it/x28vd3vv3lja1.png', headers={'User-Agent': 'Chrome'})\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweet():\n",
    "    # Retrieve one row from reddit_post\n",
    "    query = '''\n",
    "                SELECT *\n",
    "                FROM reddit_post\n",
    "                WHERE tweet_url IS NULL AND tweet_date IS NULL\n",
    "                LIMIT 1\n",
    "            '''\n",
    "    res = cur.execute(query).fetchall()\n",
    "    post_id = (res[0][0])\n",
    "\n",
    "    # Retrieve all media belonging to that post from post_media\n",
    "    query = '''\n",
    "                SELECT *\n",
    "                FROM post_media\n",
    "                WHERE post_id = ?\n",
    "            '''\n",
    "    # cur.execute(query, (post_id,))\n",
    "    for row in cur.execute(query, (post_id,)):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "TWEET_CHARACTER_LIMIT = 280\n",
    "TWEET_URL_LENGTH = 23\n",
    "TWEET_STRUCTURE_LENGTH = 8\n",
    "\n",
    "\n",
    "width = TWEET_CHARACTER_LIMIT - TWEET_URL_LENGTH - len(hashtag) - TWEET_STRUCTURE_LENGTH\n",
    "title = title\n",
    "title_wrapped = textwrap.shorten(title, width, placeholder='...')\n",
    "tweet_text = f\"\"\"\n",
    "{title_wrapped} #{hashtag}\n",
    "\n",
    "OP: {submission_url}\n",
    "\"\"\"\n",
    "tweet_text = tweet_text[1:-1]\n",
    "print(tweet_text)\n",
    "print()\n",
    "print(f' Text length: {len(tweet_text[1:-1])}')\n",
    "print(f'Tweet length: {len(tweet_text[1:-1]) - len(submission_url) + TWEET_URL_LENGTH}')\n",
    "print(width)\n",
    "print(width + TWEET_URL_LENGTH + len(hashtag) + 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SQLite tools ###\n",
    "\n",
    "for row in cur.execute(\"SELECT * FROM reddit_post\"):\n",
    "    print(row)\n",
    "\n",
    "for row in cur.execute(\"SELECT rowid, * FROM post_media\"):\n",
    "    print(row)\n",
    "\n",
    "\n",
    "# cur.execute(\"PRAGMA foreign_keys = ON\")\n",
    "# con.commit()\n",
    "# cur.execute(\"DELETE FROM reddit_post WHERE post_id = 'jbu197'\")\n",
    "# con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DateTime and Unix tools ###\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# UTC\n",
    "print(datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "# Unix from UTC\n",
    "print(int(datetime.now(timezone.utc).timestamp()))\n",
    "\n",
    "# res = cur.execute('SELECT datetime(\"1677401305.0\")')\n",
    "# res = cur.execute('SELECT datetime()')\n",
    "res = cur.execute('SELECT DATETIME(1677363730.0, \"unixepoch\")')\n",
    "print(res.fetchall()[0][0])\n",
    "\n",
    "# SQLITE    2023-02-26 21:05:22\n",
    "# PYTHON    2023-02-26 18:05:22\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "\n",
    "unix_time = 1677363730.0\n",
    "datetime_obj = datetime.fromtimestamp(unix_time) #+ timedelta(hours = 3)\n",
    "\n",
    "\n",
    "print(datetime_obj)\n",
    "\n",
    "dt = datetime.now(timezone.utc)\n",
    "print(dt.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a60f6464117e5c02b76173259f6f8a0f76e083cf7eb6246def704d393161af22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
