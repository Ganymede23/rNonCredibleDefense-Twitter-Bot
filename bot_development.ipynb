{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Authentication\n",
    "\n",
    "import tweepy\n",
    "from twitter_authentication import API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    "\n",
    "oauth1_user_handler = tweepy.OAuth1UserHandler(\n",
    "    API_KEY, API_SECRET,\n",
    "    callback=\"oob\"\n",
    ")\n",
    "\n",
    "print(oauth1_user_handler.get_authorization_url())\n",
    "\n",
    "verifier = input(\"Input PIN: \")\n",
    "access_token, access_token_secret = oauth1_user_handler.get_access_token(\n",
    "    verifier\n",
    ")\n",
    "# print(access_token)\n",
    "# print(access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    "\n",
    "# API\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Client\n",
    "client = tweepy.Client(\n",
    "    consumer_key = API_KEY,\n",
    "    consumer_secret = API_SECRET,\n",
    "    access_token = ACCESS_TOKEN,\n",
    "    access_token_secret = ACCESS_TOKEN_SECRET,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Tweet Test\n",
    "\n",
    "media = api.media_upload(\"Media/F-35 LOGO.png\")\n",
    "# print(media)\n",
    "\n",
    "client.create_tweet(text='t4', media_ids=[media.media_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit authentication\n",
    "\n",
    "import praw\n",
    "from reddit_authentication import CLIENT_ID, SECRET_KEY, USER_AGENT\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = CLIENT_ID,\n",
    "    client_secret = SECRET_KEY,\n",
    "    user_agent = USER_AGENT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLITE\n",
    "\n",
    "import sqlite3\n",
    "con = sqlite3.connect(\"bot_db.db\")\n",
    "cur = con.cursor()\n",
    "cur.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS reddit_post (\n",
    "                    post_id     TEXT PRIMARY KEY,\n",
    "                    title       TEXT NOT NULL,\n",
    "                    flair       TEXT,\n",
    "                    post_url    TEXT NOT NULL,\n",
    "                    author      TEXT NOT NULL,\n",
    "                    post_date   TEXT NOT NULL,\n",
    "                    scrape_date TEXT NOT NULL,\n",
    "                    tweet_url   TEXT,\n",
    "                    tweet_date  TEXT\n",
    "            );''')\n",
    "\n",
    "def insert_into_table(db: str, table_name: str, data: list):\n",
    "    con = sqlite3.connect(db)\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    sql = f\"INSERT OR IGNORE INTO {table_name} VALUES ({','.join(['?' for _ in data[0]])})\"\n",
    "    cur.executemany(sql, data)\n",
    "    \n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "def update_table(db: str, table_name: str, set_values_dict: dict, where_clause: str):\n",
    "    con = sqlite3.connect(db)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    set_clause = ', '.join([f\"{col} = ?\" for col in set_values_dict.keys()])\n",
    "    sql = f\"UPDATE {table_name} SET {set_clause} WHERE {where_clause}\"\n",
    "    cur.execute(sql, tuple(set_values_dict.values()))\n",
    "\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded cell to test specific posts\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "post_data = []\n",
    "submission = reddit.submission(url='https://www.reddit.com/r/NonCredibleDefense/comments/11rhzex/my_recreation_of_the_events_in_the_black_sea_today/')\n",
    "\n",
    "if not submission.stickied and not submission.is_self:\n",
    "    # print(f'Post: {submission.id, submission.name, submission.title, submission.selftext}')\n",
    "    # print(f'Flair: {submission.link_flair_text}')\n",
    "    # print(f'Status: {submission.over_18, submission.spoiler, submission.stickied, submission.created_utc}')\n",
    "    # print(f'Links: reddit.com{submission.permalink}, {submission.url}')\n",
    "    # print(f'Author: {submission.author.name}')\n",
    "    # print(submission.score)\n",
    "    \n",
    "    # print(submission.is_video)\n",
    "    # print(submission.media)\n",
    "    # print(submission.url)\n",
    "\n",
    "    post_date = datetime.fromtimestamp(submission.created_utc) + timedelta(hours = 3) # Adds 3 hours because local time is GMT-3. Review this.\n",
    "    datetime_now = datetime.now(timezone.utc)\n",
    "    scrape_date = datetime_now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    post_data_row = (\n",
    "            submission.id,                          # post_id     TEXT PRIMARY KEY,\n",
    "            submission.title,                       # title       TEXT NOT NULL,\n",
    "            submission.link_flair_text,             # flair       TEXT,\n",
    "            'reddit.com' + submission.permalink,    # post_url    TEXT NOT NULL,\n",
    "            submission.author.name,                 # author      TEXT NOT NULL,\n",
    "            submission.created_utc,                 # post_date   TEXT NOT NULL,\n",
    "            scrape_date,                            # scrape_date TEXT NOT NULL,\n",
    "            None,\n",
    "            None\n",
    "    )\n",
    "    post_data.append(post_data_row)\n",
    "\n",
    "# insert_into_table('bot_db.db', 'reddit_post', post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "post_data = []\n",
    "\n",
    "for submission in reddit.subreddit(\"NonCredibleDefense\").top(limit = 5, time_filter = \"day\"):\n",
    "    if not submission.stickied and not submission.is_self:\n",
    "        # print(f'Post: {submission.id, submission.name, submission.title, submission.selftext}')\n",
    "        # print(f'Flair: {submission.link_flair_text}')\n",
    "        # print(f'Status: {submission.over_18, submission.spoiler, submission.stickied, submission.created_utc}')\n",
    "        # print(f'Links: reddit.com{submission.permalink}, {submission.url}')\n",
    "        # print(f'Author: {submission.author.name}')\n",
    "        # print(submission.score)\n",
    "\n",
    "        post_date = datetime.fromtimestamp(submission.created_utc) + timedelta(hours = 3) # Adds 3 hours because local time is GMT-3. Review this.\n",
    "        datetime_now = datetime.now(timezone.utc)\n",
    "        scrape_date = datetime_now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        post_data_row = (\n",
    "                submission.id,                          # post_id     TEXT PRIMARY KEY,\n",
    "                submission.title,                       # title       TEXT NOT NULL,\n",
    "                submission.link_flair_text,             # flair       TEXT,\n",
    "                'https://www.reddit.com' + submission.permalink,    # post_url    TEXT NOT NULL,\n",
    "                submission.author.name,                 # author      TEXT NOT NULL,\n",
    "                submission.created_utc,                 # post_date   TEXT NOT NULL,\n",
    "                scrape_date,                            # scrape_date TEXT NOT NULL,\n",
    "                None,\n",
    "                None\n",
    "        )\n",
    "        post_data.append(post_data_row)\n",
    "        print(post_data_row)\n",
    "\n",
    "        # if hasattr(submission, 'gallery_data'):\n",
    "        #     for item in sorted(submission.gallery_data['items'], key=lambda x: x['id']):\n",
    "        #         media_id = item['media_id']\n",
    "        #         media_metadata = submission.media_metadata[media_id]\n",
    "        #         print(media_metadata['s']['u'])\n",
    "        #         media_data_row = (\n",
    "        #                 submission.id,                          # post_id     TEXT NOT NULL,\n",
    "        #                 media_metadata['s']['u'],               # url         TEXT NOT NULL,\n",
    "        #         )\n",
    "        #         # print(media_data_row)\n",
    "        #         media_data.append(media_data_row)\n",
    "        # else:\n",
    "        #     media_data_row = (\n",
    "        #             submission.id,                          # post_id     TEXT NOT NULL,\n",
    "        #             submission.url,                         # url         TEXT NOT NULL,\n",
    "        #     )\n",
    "        #     media_data.append(media_data_row)\n",
    "        # print(media_data)\n",
    "\n",
    "insert_into_table('bot_db.db', 'reddit_post', post_data)\n",
    "\n",
    "# tweet_url   TEXT,\n",
    "# tweet_date  TEXT\n",
    "\n",
    "        # title = submission.title\n",
    "        # flair = submission.link_flair_text\n",
    "        # if flair:\n",
    "        #     flair = flair.translate(str.maketrans('&', 'n', '!/ '))\n",
    "        # submission_url = 'reddit.com' + submission.permalink\n",
    "        # media_url = submission.url\n",
    "        # author = submission.author.name\n",
    "        # user_profile_url = 'reddit.com/u/' + user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.reddit.com/r/dogelore/comments/jbu197/he_chose_poorly/'\n",
    "# submission = reddit.submission(url=url)\n",
    "\n",
    "url_list = [\n",
    "    'https://www.reddit.com/r/dogelore/comments/jbu197/he_chose_poorly/', \n",
    "    'https://www.reddit.com/r/NonCredibleDefense/comments/t0o3lc/godspeed_ghost_of_kyiv_may_the_razgriz_be_with_you/', \n",
    "    'https://www.reddit.com/r/NonCredibleDefense/comments/11bimqt/pov_its_20220225_second_day_kyiv_time_you_are/'\n",
    "]\n",
    "\n",
    "for item in url_list:\n",
    "    submission = reddit.submission(url=item)\n",
    "    print(hasattr(submission, 'gallery_data'))\n",
    "    # for item in sorted(submission.gallery_data['items'], key=lambda x: x['id']):\n",
    "    #     print(f'Post: {submission.id, submission.name, submission.title, submission.selftext}')\n",
    "    #     print(f'Flair: {submission.link_flair_text}')\n",
    "    #     print(f'Status: {submission.over_18, submission.spoiler, submission.stickied, submission.created_utc}')\n",
    "    #     print(f'Links: reddit.com{submission.permalink}, {submission.url}')\n",
    "    #     print(f'Author: {submission.author.name}')\n",
    "    #     print()\n",
    "\n",
    "# GALLERY CASE\n",
    "\n",
    "# url = 'https://www.reddit.com/r/NonCredibleDefense/comments/11bimqt/pov_its_20220225_second_day_kyiv_time_you_are/'\n",
    "url = 'https://www.reddit.com/r/dogelore/comments/jbu197/he_chose_poorly/'\n",
    "# url = 'https://www.reddit.com/r/NonCredibleDefense/comments/11pkssc/any_day_now/'\n",
    "submission = reddit.submission(url=url)\n",
    "# print(len(sorted(submission.gallery_data['items'], key=lambda x: x['id'])))\n",
    "for item in sorted(submission.gallery_data['items'], key=lambda x: x['id']):\n",
    "    media_id = item['media_id']\n",
    "    media_metadata = submission.media_metadata[media_id]\n",
    "    print(media_metadata['s']['u'])\n",
    "    print(type(media_metadata['s']['u']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "MEDIA_FOLDER_PATH = './temp_media/'\n",
    "\n",
    "def clear_media_folder():\n",
    "    folder_path = MEDIA_FOLDER_PATH\n",
    "    try:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "            else:\n",
    "                shutil.rmtree(file_path)\n",
    "    except:\n",
    "        print(\"An exception occurred when trying to clear the media folder.\")\n",
    "\n",
    "# clear_media_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RedDownloader import RedDownloader\n",
    "\n",
    "url = 'https://www.reddit.com/r/NonCredibleDefense/comments/11sbyta/things_have_really_changed_in_the_past_20_years/'\n",
    "\n",
    "def download_media_from_reddit(url: str):\n",
    "    clear_media_folder()\n",
    "    file = RedDownloader.Download(url, quality=1080, destination=MEDIA_FOLDER_PATH)\n",
    "    # print(file.GetMediaType())\n",
    "\n",
    "# download_media_from_reddit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MEDIA_FOLDER_PATH = './temp_media/'\n",
    "\n",
    "def upload_media_to_twitter():\n",
    "    media_ids = []\n",
    "    for path, subdirs, files in os.walk(MEDIA_FOLDER_PATH):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(path, name)\n",
    "            try:\n",
    "                media = api.media_upload(file_path)\n",
    "                media_ids.append(media.media_id)\n",
    "                # print(media.media_id)\n",
    "            except:\n",
    "                print(f\"An exception occurred when trying to upload this media file to Twitter: {file_path}.\")\n",
    "    \n",
    "    return media_ids\n",
    "    \n",
    "\n",
    "# def upload_media():\n",
    "#     folder_path = MEDIA_FOLDER_PATH\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         print(file_path)\n",
    "\n",
    "# upload_media()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from time import sleep\n",
    "\n",
    "TWEET_CHARACTER_LIMIT = 280\n",
    "TWEET_URL_LENGTH = 23\n",
    "TWEET_STRUCTURE_LENGTH = 8\n",
    "\n",
    "def create_tweet():\n",
    "    # Retrieve one row from reddit_post\n",
    "    query = '''\n",
    "                SELECT *\n",
    "                FROM reddit_post\n",
    "                WHERE tweet_url IS NULL AND tweet_date IS NULL\n",
    "                LIMIT 1\n",
    "            '''\n",
    "    res = cur.execute(query).fetchall()\n",
    "    post_id, title, flair, post_url, author, post_date, scrape_date, tweet_url, tweet_date = res[0]\n",
    "    print(res)\n",
    "\n",
    "    download_media_from_reddit(post_url)\n",
    "\n",
    "    # flair = 'test1'\n",
    "    # title = 'title test'\n",
    "    # post_url = 'https://url_test.com'\n",
    "\n",
    "    media_ids = upload_media_to_twitter()\n",
    "\n",
    "    # Get rid of spaces and other characters in the flair and turn it into a hashtag\n",
    "    if flair:\n",
    "        flair = flair.translate(str.maketrans('&', 'n', '!/ '))\n",
    "\n",
    "    width = TWEET_CHARACTER_LIMIT - TWEET_URL_LENGTH - len(flair) - TWEET_STRUCTURE_LENGTH\n",
    "    \n",
    "    # Wrap the title in case it's too long\n",
    "    title_wrapped = textwrap.shorten(title, width, placeholder='...')\n",
    "    \n",
    "    tweet_text = f\"\"\"\n",
    "    {title_wrapped} #{flair}\\n\\nOP: {post_url}\n",
    "    \"\"\"\n",
    "    tweet_text = tweet_text[1:-1]\n",
    "\n",
    "    # Tweet creation\n",
    "    if len(media_ids) <= 4:     # If 4 media items or less, it creates the tweet\n",
    "        response = client.create_tweet(text=tweet_text, media_ids=media_ids)\n",
    "    else:                       # If more than 4 media items, it posts the first 4 images and then creates a thread replying the last tweet.\n",
    "        response = client.create_tweet(text=tweet_text, media_ids=media_ids[:4])\n",
    "        previous_tweet_id = response[0]['id']\n",
    "        media_ids = media_ids[4:]\n",
    "        while len(media_ids) > 0:\n",
    "            sleep(5)\n",
    "            response = client.create_tweet(in_reply_to_tweet_id=previous_tweet_id, media_ids=media_ids[:4])\n",
    "            previous_tweet_id = response[0]['id']\n",
    "            media_ids = media_ids[4:]\n",
    "\n",
    "create_tweet()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# title = submission.title\n",
    "# flair = submission.link_flair_text\n",
    "\n",
    "# submission_url = 'reddit.com' + submission.permalink\n",
    "# media_url = submission.url\n",
    "# author = submission.author.name\n",
    "# user_profile_url = 'reddit.com/u/' + user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "TWEET_CHARACTER_LIMIT = 280\n",
    "TWEET_URL_LENGTH = 23\n",
    "TWEET_STRUCTURE_LENGTH = 8\n",
    "\n",
    "\n",
    "width = TWEET_CHARACTER_LIMIT - TWEET_URL_LENGTH - len(hashtag) - TWEET_STRUCTURE_LENGTH\n",
    "title = title\n",
    "title_wrapped = textwrap.shorten(title, width, placeholder='...')\n",
    "tweet_text = f\"\"\"\n",
    "{title_wrapped} #{hashtag}\n",
    "\n",
    "OP: {submission_url}\n",
    "\"\"\"\n",
    "tweet_text = tweet_text[1:-1]\n",
    "print(tweet_text)\n",
    "print()\n",
    "print(f' Text length: {len(tweet_text[1:-1])}')\n",
    "print(f'Tweet length: {len(tweet_text[1:-1]) - len(submission_url) + TWEET_URL_LENGTH}')\n",
    "print(width)\n",
    "print(width + TWEET_URL_LENGTH + len(hashtag) + 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://praw.readthedocs.io/en/stable/getting_started/quick_start.html\n",
    "# https://github.com/reddit-archive/reddit/wiki/API#rules\n",
    "# https://docs.python.org/3/library/sqlite3.html\n",
    "\n",
    "# VIDEO CASES\n",
    "# url = 'https://www.reddit.com/r/NonCredibleDefense/comments/11b2sxa/i_think_ukrainian_minister_of_defense_just/'\n",
    "# url = 'https://www.reddit.com/r/NonCredibleDefense/comments/t0o3lc/godspeed_ghost_of_kyiv_may_the_razgriz_be_with_you/'\n",
    "\n",
    "# GALLERY CASES\n",
    "# url = 'https://www.reddit.com/r/dogelore/comments/jbu197/he_chose_poorly/'\n",
    "# url = 'https://www.reddit.com/r/announcements/comments/hrrh23/now_you_can_make_posts_with_multiple_images/'\n",
    "# url = 'https://www.reddit.com/r/hvacadvice/comments/x7cep9/black_soot_at_electric_box/'\n",
    "# url = 'https://www.reddit.com/r/ProCreate/comments/10wdd7c/gallery_post_description_in_the_comment/'\n",
    "\n",
    "# SPECIAL GALLERY CASE\n",
    "# url = 'https://www.reddit.com/r/NonCredibleDefense/comments/11bimqt/pov_its_20220225_second_day_kyiv_time_you_are/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SQLite tools ###\n",
    "\n",
    "for row in cur.execute(\"SELECT * FROM reddit_post\"):\n",
    "    print(row)\n",
    "\n",
    "# for row in cur.execute(\"SELECT rowid, * FROM post_media\"):\n",
    "#     print(row)\n",
    "\n",
    "# cur.execute(query, (post_id,))\n",
    "# for row in cur.execute(query, (post_id,)):\n",
    "#     print(row)\n",
    "\n",
    "# cur.execute(\"PRAGMA foreign_keys = ON\")\n",
    "# con.commit()\n",
    "# cur.execute(\"DELETE FROM reddit_post WHERE post_id = 'jbu197'\")\n",
    "# con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DateTime and Unix tools ###\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# UTC\n",
    "print(datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "# Unix from UTC\n",
    "print(int(datetime.now(timezone.utc).timestamp()))\n",
    "\n",
    "# res = cur.execute('SELECT datetime(\"1677401305.0\")')\n",
    "# res = cur.execute('SELECT datetime()')\n",
    "res = cur.execute('SELECT DATETIME(1677363730.0, \"unixepoch\")')\n",
    "print(res.fetchall()[0][0])\n",
    "\n",
    "# SQLITE    2023-02-26 21:05:22\n",
    "# PYTHON    2023-02-26 18:05:22\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "\n",
    "unix_time = 1677363730.0\n",
    "datetime_obj = datetime.fromtimestamp(unix_time) #+ timedelta(hours = 3)\n",
    "\n",
    "\n",
    "print(datetime_obj)\n",
    "\n",
    "dt = datetime.now(timezone.utc)\n",
    "print(dt.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a60f6464117e5c02b76173259f6f8a0f76e083cf7eb6246def704d393161af22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
